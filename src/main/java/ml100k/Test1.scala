package ml100k

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}


object Test1 {
  def main(args: Array[String]): Unit = {

    val context = SparkContext.getOrCreate(new SparkConf().setAppName("li").setMaster("local[*]"))

    // æ•°æ®çš„å­—æ®µ
    // userid (ç”¨æˆ·ğŸ†”id) ã€item id (é¡¹ç›®id) ã€ rating(è¯„åˆ†)ã€timestamp(æ—¥æœŸæ—¶é—´)
    val value: RDD[String] = context.textFile("/home/rjxy/IdeaProjects/spark/spark_mllib_course/src/main/resources/ml-100k/u.data")

    value.take(10).foreach(println)

    //è·å–å­—æ®µçš„çŠ¶æ€
    //- count è®¡æ•°
    //- mean å¹³å‡
    //- stdev æ ‡å‡†åå·®
    //- max æœ€å¤§å€¼
    //- min æœ€å°å€¼
    println("UserId"+value.map(_.split("\t")(0).toDouble).stats())
    println("ItemId"+value.map(_.split("\t")(1).toDouble).stats())
    println("rateId"+value.map(_.split("\t")(2).toDouble).stats())
    println("timestamp"+value.map(_.split("\t")(3).toDouble).stats())
    println("timestamp"+value.map(_.split("\t")(3).toDouble).stats())


    //


   context.stop()

  }
}
